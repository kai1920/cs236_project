#@title base GAN

# Hyperparameters
learning_rate = 0.0002
batch_size = 32
epochs = 3
generator_name = 'base_generator'
discriminator_name = 'base_discriminator'

# Initialize generator and discriminator
generator = Generator()
discriminator = Discriminator()

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
validation_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)

# Loss and Optimizer
criterion = nn.BCELoss()
optimizer_g = torch.optim.Adam(generator.parameters(), lr=learning_rate)
optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)


for epoch in range(epochs):
    for i, (real_data, _) in enumerate(train_loader):
        current_batch_size = real_data.size(0)

        # Flatten the real data
        real_data = real_data.view(current_batch_size, -1)

        # Train Discriminator with real data
        optimizer_d.zero_grad()
        real_labels = torch.ones(current_batch_size, 1)
        real_loss = criterion(discriminator(real_data), real_labels)
        real_loss.backward()

        # Generate fake data
        z = torch.randn(current_batch_size, 768)
        generator.eval()  # Set generator to evaluation mode
        fake_data = generator(z)
        generator.train()  # Set generator back to training mode

        # Train Discriminator with fake data
        fake_labels = torch.zeros(current_batch_size, 1)
        fake_loss = criterion(discriminator(fake_data.detach()), fake_labels)
        fake_loss.backward()
        optimizer_d.step()

        # Train Generator
        optimizer_g.zero_grad()
        validity = discriminator(fake_data)
        g_loss = criterion(validity, torch.ones(current_batch_size, 1))
        g_loss.backward()
        optimizer_g.step()


    # Logging, etc.
    print(f"Epoch {epoch+1}/{epochs}, D Loss: {real_loss + fake_loss}, G Loss: {g_loss}")

torch.save(generator, os.path.join(model_path,f'{generator_name}.pth'))
torch.save(discriminator, os.path.join(model_path,f'{discriminator_name}.pth'))
